{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"DQN Network\n",
    "        Args:\n",
    "            input_dim (int): `state` dimension.\n",
    "                `state` is 2-D tensor of shape (n, input_dim)\n",
    "            output_dim (int): Number of actions.\n",
    "                Q_value is 2-D tensor of shape (n, output_dim)\n",
    "            hidden_dim (int): Hidden dimension in fc layer\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.final = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\",\n",
    "                        field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Replay memory class\n",
    "        Args:\n",
    "            capacity (int): Max size of this memory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.cursor = 0\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: int,\n",
    "             next_state: np.ndarray,\n",
    "             done: bool) -> None:\n",
    "        \"\"\"Creates `Transition` and insert\n",
    "        Args:\n",
    "            state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            action (int): action index (0 <= action < output_dim)\n",
    "            reward (int): reward value\n",
    "            next_state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            done (bool): whether this state was last step\n",
    "        \"\"\"\n",
    "        if len(self) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.cursor] = Transition(state,\n",
    "                                              action, reward, next_state, done)\n",
    "        self.cursor = (self.cursor + 1) % self.capacity\n",
    "\n",
    "    def pop(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Returns a minibatch of `Transition` randomly\n",
    "        Args:\n",
    "            batch_size (int): Size of mini-bach\n",
    "        Returns:\n",
    "            List[Transition]: Minibatch of `Transition`\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"Agent class that choose action and train\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden dimension\n",
    "        \"\"\"\n",
    "        self.dqn = DQN(input_dim, output_dim, hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.dqn.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"torch.Variable syntax helper\n",
    "        Args:\n",
    "            x (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: torch variable\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray, eps: float) -> int:\n",
    "        \"\"\"Returns an action\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "            eps (float): ùú∫-greedy for exploration\n",
    "        Returns:\n",
    "            int: action index\n",
    "        \"\"\"\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.choice(self.output_dim)\n",
    "        else:\n",
    "            self.dqn.train(mode=False)\n",
    "            scores = self.get_Q(states)\n",
    "            _, argmax = torch.max(scores.data, 1)\n",
    "            return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        \"\"\"Returns `Q-value`\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D Tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.FloatTensor: 2-D Tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        states = self._to_variable(states.reshape(-1, self.input_dim))\n",
    "        self.dqn.train(mode=False)\n",
    "        return self.dqn(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            Q_pred (torch.FloatTensor): Predicted value by the network,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "            Q_true (torch.FloatTensor): Target value obtained from the game,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        self.dqn.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "def train_helper(agent: Agent, minibatch: List[Transition], gamma: float) -> float:\n",
    "    \"\"\"Prepare minibatch and train them\n",
    "    Args:\n",
    "        agent (Agent): Agent has `train(Q_pred, Q_true)` method\n",
    "        minibatch (List[Transition]): Minibatch of `Transition`\n",
    "        gamma (float): Discount rate of Q_target\n",
    "    Returns:\n",
    "        float: Loss value\n",
    "    \"\"\"\n",
    "    states = np.vstack([x.state for x in minibatch])\n",
    "    actions = np.array([x.action for x in minibatch])\n",
    "    rewards = np.array([x.reward for x in minibatch])\n",
    "    next_states = np.vstack([x.next_state for x in minibatch])\n",
    "    done = np.array([x.done for x in minibatch])\n",
    "\n",
    "    Q_predict = agent.get_Q(states)\n",
    "    Q_target = Q_predict.clone().data.numpy()\n",
    "    Q_target[np.arange(len(Q_target)), actions] = rewards + gamma * np.max(agent.get_Q(next_states).data.numpy(), axis=1) * ~done\n",
    "    Q_target = agent._to_variable(Q_target)\n",
    "\n",
    "    return agent.train(Q_predict, Q_target)\n",
    "\n",
    "\n",
    "def play_episode(env: gym.Env,\n",
    "                 agent: Agent,\n",
    "                 replay_memory: ReplayMemory,\n",
    "                 eps: float,\n",
    "                 batch_size: int) -> int:\n",
    "    \"\"\"Play an epsiode and train\n",
    "    Args:\n",
    "        env (gym.Env): gym environment (CartPole-v0)\n",
    "        agent (Agent): agent will train and get action\n",
    "        replay_memory (ReplayMemory): trajectory is saved here\n",
    "        eps (float): ùú∫-greedy for exploration\n",
    "        batch_size (int): batch size\n",
    "    Returns:\n",
    "        int: reward earned in this episode\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "        env.render()\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "        if done:\n",
    "            r = -1\n",
    "        replay_memory.push(s, a, r, s2, done)\n",
    "\n",
    "        if len(replay_memory) > batch_size:\n",
    "\n",
    "            minibatch = replay_memory.pop(batch_size)\n",
    "            train_helper(agent, minibatch, GAMMA)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def get_env_dim(env: gym.Env) -> Tuple[int, int]:\n",
    "    \"\"\"Returns input_dim & output_dim\n",
    "    Args:\n",
    "        env (gym.Env): gym Environment (CartPole-v0)\n",
    "    Returns:\n",
    "        int: input_dim\n",
    "        int: output_dim\n",
    "    \"\"\"\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    return input_dim, output_dim\n",
    "\n",
    "\n",
    "def epsilon_annealing(epsiode: int, max_episode: int, min_eps: float) -> float:\n",
    "    \"\"\"Returns ùú∫-greedy\n",
    "    1.0---|\\\n",
    "          | \\\n",
    "          |  \\\n",
    "    min_e +---+------->\n",
    "              |\n",
    "              max_episode\n",
    "    Args:\n",
    "        epsiode (int): Current episode (0<= episode)\n",
    "        max_episode (int): After max episode, ùú∫ will be `min_eps`\n",
    "        min_eps (float): ùú∫ will never go below this value\n",
    "    Returns:\n",
    "        float: ùú∫ value\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_eps - 1.0) / max_episode\n",
    "    return max(slope * epsiode + 1.0, min_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode:     1] Reward:  11.0 ùú∫-greedy:  1.00\n",
      "[Episode:     2] Reward:  21.0 ùú∫-greedy:  0.98\n",
      "[Episode:     3] Reward:  32.0 ùú∫-greedy:  0.96\n",
      "[Episode:     4] Reward:  17.0 ùú∫-greedy:  0.94\n",
      "[Episode:     5] Reward:  18.0 ùú∫-greedy:  0.92\n",
      "[Episode:     6] Reward:  28.0 ùú∫-greedy:  0.90\n",
      "[Episode:     7] Reward:  14.0 ùú∫-greedy:  0.88\n",
      "[Episode:     8] Reward:  20.0 ùú∫-greedy:  0.86\n",
      "[Episode:     9] Reward:  15.0 ùú∫-greedy:  0.84\n",
      "[Episode:    10] Reward:  10.0 ùú∫-greedy:  0.82\n",
      "[Episode:    11] Reward:  13.0 ùú∫-greedy:  0.80\n",
      "[Episode:    12] Reward:  10.0 ùú∫-greedy:  0.78\n",
      "[Episode:    13] Reward:  24.0 ùú∫-greedy:  0.76\n",
      "[Episode:    14] Reward:  10.0 ùú∫-greedy:  0.74\n",
      "[Episode:    15] Reward:  21.0 ùú∫-greedy:  0.72\n",
      "[Episode:    16] Reward:  29.0 ùú∫-greedy:  0.70\n",
      "[Episode:    17] Reward:  22.0 ùú∫-greedy:  0.68\n",
      "[Episode:    18] Reward:  16.0 ùú∫-greedy:  0.66\n",
      "[Episode:    19] Reward:  13.0 ùú∫-greedy:  0.64\n",
      "[Episode:    20] Reward:  14.0 ùú∫-greedy:  0.62\n",
      "[Episode:    21] Reward:  15.0 ùú∫-greedy:  0.60\n",
      "[Episode:    22] Reward:  11.0 ùú∫-greedy:  0.58\n",
      "[Episode:    23] Reward:  11.0 ùú∫-greedy:  0.56\n",
      "[Episode:    24] Reward:   9.0 ùú∫-greedy:  0.54\n",
      "[Episode:    25] Reward:  12.0 ùú∫-greedy:  0.52\n",
      "[Episode:    26] Reward:   9.0 ùú∫-greedy:  0.51\n",
      "[Episode:    27] Reward:  12.0 ùú∫-greedy:  0.49\n",
      "[Episode:    28] Reward:  11.0 ùú∫-greedy:  0.47\n",
      "[Episode:    29] Reward:  10.0 ùú∫-greedy:  0.45\n",
      "[Episode:    30] Reward:  10.0 ùú∫-greedy:  0.43\n",
      "[Episode:    31] Reward:  17.0 ùú∫-greedy:  0.41\n",
      "[Episode:    32] Reward:  11.0 ùú∫-greedy:  0.39\n",
      "[Episode:    33] Reward:  12.0 ùú∫-greedy:  0.37\n",
      "[Episode:    34] Reward:  11.0 ùú∫-greedy:  0.35\n",
      "[Episode:    35] Reward:   9.0 ùú∫-greedy:  0.33\n",
      "[Episode:    36] Reward:  15.0 ùú∫-greedy:  0.31\n",
      "[Episode:    37] Reward:   9.0 ùú∫-greedy:  0.29\n",
      "[Episode:    38] Reward:  11.0 ùú∫-greedy:  0.27\n",
      "[Episode:    39] Reward:  10.0 ùú∫-greedy:  0.25\n",
      "[Episode:    40] Reward:   9.0 ùú∫-greedy:  0.23\n",
      "[Episode:    41] Reward:  63.0 ùú∫-greedy:  0.21\n",
      "[Episode:    42] Reward:  15.0 ùú∫-greedy:  0.19\n",
      "[Episode:    43] Reward:  12.0 ùú∫-greedy:  0.17\n",
      "[Episode:    44] Reward:  47.0 ùú∫-greedy:  0.15\n",
      "[Episode:    45] Reward:  19.0 ùú∫-greedy:  0.13\n",
      "[Episode:    46] Reward:  14.0 ùú∫-greedy:  0.11\n",
      "[Episode:    47] Reward:  11.0 ùú∫-greedy:  0.09\n",
      "[Episode:    48] Reward:  18.0 ùú∫-greedy:  0.07\n",
      "[Episode:    49] Reward:  29.0 ùú∫-greedy:  0.05\n",
      "[Episode:    50] Reward:  13.0 ùú∫-greedy:  0.03\n",
      "[Episode:    51] Reward:  11.0 ùú∫-greedy:  0.01\n",
      "[Episode:    52] Reward:  11.0 ùú∫-greedy:  0.01\n",
      "[Episode:    53] Reward:  16.0 ùú∫-greedy:  0.01\n",
      "[Episode:    54] Reward:  13.0 ùú∫-greedy:  0.01\n",
      "[Episode:    55] Reward:  12.0 ùú∫-greedy:  0.01\n",
      "[Episode:    56] Reward:  29.0 ùú∫-greedy:  0.01\n",
      "[Episode:    57] Reward:  58.0 ùú∫-greedy:  0.01\n",
      "[Episode:    58] Reward:  14.0 ùú∫-greedy:  0.01\n",
      "[Episode:    59] Reward:  38.0 ùú∫-greedy:  0.01\n",
      "[Episode:    60] Reward:  26.0 ùú∫-greedy:  0.01\n",
      "[Episode:    61] Reward:  27.0 ùú∫-greedy:  0.01\n",
      "[Episode:    62] Reward:  26.0 ùú∫-greedy:  0.01\n",
      "[Episode:    63] Reward:  27.0 ùú∫-greedy:  0.01\n",
      "[Episode:    64] Reward:  24.0 ùú∫-greedy:  0.01\n",
      "[Episode:    65] Reward: 122.0 ùú∫-greedy:  0.01\n",
      "[Episode:    66] Reward:  45.0 ùú∫-greedy:  0.01\n",
      "[Episode:    67] Reward:  43.0 ùú∫-greedy:  0.01\n",
      "[Episode:    68] Reward:  25.0 ùú∫-greedy:  0.01\n",
      "[Episode:    69] Reward:  35.0 ùú∫-greedy:  0.01\n",
      "[Episode:    70] Reward:  39.0 ùú∫-greedy:  0.01\n",
      "[Episode:    71] Reward:  19.0 ùú∫-greedy:  0.01\n",
      "[Episode:    72] Reward:  38.0 ùú∫-greedy:  0.01\n",
      "[Episode:    73] Reward:  45.0 ùú∫-greedy:  0.01\n",
      "[Episode:    74] Reward: 116.0 ùú∫-greedy:  0.01\n",
      "[Episode:    75] Reward:  25.0 ùú∫-greedy:  0.01\n",
      "[Episode:    76] Reward:  51.0 ùú∫-greedy:  0.01\n",
      "[Episode:    77] Reward:  47.0 ùú∫-greedy:  0.01\n",
      "[Episode:    78] Reward:  58.0 ùú∫-greedy:  0.01\n",
      "[Episode:    79] Reward: 118.0 ùú∫-greedy:  0.01\n",
      "[Episode:    80] Reward:  61.0 ùú∫-greedy:  0.01\n",
      "[Episode:    81] Reward:  41.0 ùú∫-greedy:  0.01\n",
      "[Episode:    82] Reward:  36.0 ùú∫-greedy:  0.01\n",
      "[Episode:    83] Reward:  64.0 ùú∫-greedy:  0.01\n",
      "[Episode:    84] Reward:  75.0 ùú∫-greedy:  0.01\n",
      "[Episode:    85] Reward:  71.0 ùú∫-greedy:  0.01\n",
      "[Episode:    86] Reward:  40.0 ùú∫-greedy:  0.01\n",
      "[Episode:    87] Reward:  86.0 ùú∫-greedy:  0.01\n",
      "[Episode:    88] Reward: 142.0 ùú∫-greedy:  0.01\n",
      "[Episode:    89] Reward:  68.0 ùú∫-greedy:  0.01\n",
      "[Episode:    90] Reward:  55.0 ùú∫-greedy:  0.01\n",
      "[Episode:    91] Reward:  62.0 ùú∫-greedy:  0.01\n",
      "[Episode:    92] Reward:  77.0 ùú∫-greedy:  0.01\n",
      "[Episode:    93] Reward:  67.0 ùú∫-greedy:  0.01\n",
      "[Episode:    94] Reward:  72.0 ùú∫-greedy:  0.01\n",
      "[Episode:    95] Reward:  76.0 ùú∫-greedy:  0.01\n",
      "[Episode:    96] Reward:  77.0 ùú∫-greedy:  0.01\n",
      "[Episode:    97] Reward:  89.0 ùú∫-greedy:  0.01\n",
      "[Episode:    98] Reward:  96.0 ùú∫-greedy:  0.01\n",
      "[Episode:    99] Reward:  95.0 ùú∫-greedy:  0.01\n",
      "[Episode:   100] Reward: 195.0 ùú∫-greedy:  0.01\n",
      "[Episode:   101] Reward:  87.0 ùú∫-greedy:  0.01\n",
      "[Episode:   102] Reward: 106.0 ùú∫-greedy:  0.01\n",
      "[Episode:   103] Reward: 121.0 ùú∫-greedy:  0.01\n",
      "[Episode:   104] Reward: 142.0 ùú∫-greedy:  0.01\n",
      "[Episode:   105] Reward: 122.0 ùú∫-greedy:  0.01\n",
      "[Episode:   106] Reward: 191.0 ùú∫-greedy:  0.01\n",
      "[Episode:   107] Reward:  66.0 ùú∫-greedy:  0.01\n",
      "[Episode:   108] Reward:  51.0 ùú∫-greedy:  0.01\n",
      "[Episode:   109] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   110] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   111] Reward: 185.0 ùú∫-greedy:  0.01\n",
      "[Episode:   112] Reward: 200.0 ùú∫-greedy:  0.01\n",
      "[Episode:   113] Reward: 200.0 ùú∫-greedy:  0.01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a254eece301a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_annealing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_EPISODE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMIN_EPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e1e55fd0174c>\u001b[0m in \u001b[0;36mplay_episode\u001b[0;34m(env, agent, replay_memory, eps, batch_size)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mminibatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mtrain_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-e1e55fd0174c>\u001b[0m in \u001b[0;36mtrain_helper\u001b[0;34m(agent, minibatch, gamma)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mQ_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a3e1cf3dce9b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, Q_pred, Q_true)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ENV = \"CartPole-v0\"\n",
    "HIDDEN_DIM = 12\n",
    "MEMORY_CAPACITY = 5000\n",
    "MAX_EPISODE = 50\n",
    "MIN_EPS = 0.01\n",
    "BATCH_SIZE = 64\n",
    "N_EPISODES = 200\n",
    "\n",
    "try:\n",
    "    env = gym.make(ENV)\n",
    "    env = gym.wrappers.Monitor(env, directory=\"monitors\", force=True)\n",
    "    rewards = deque(maxlen=100)\n",
    "    input_dim, output_dim = get_env_dim(env)\n",
    "    agent = Agent(input_dim, output_dim, HIDDEN_DIM)\n",
    "    replay_memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "    for i in range(N_EPISODES):\n",
    "        eps = epsilon_annealing(i, MAX_EPISODE, MIN_EPS)\n",
    "        r = play_episode(env, agent, replay_memory, eps, BATCH_SIZE)\n",
    "        print(\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\".format(i + 1, r, eps))\n",
    "\n",
    "        rewards.append(r)\n",
    "\n",
    "        if len(rewards) == rewards.maxlen:\n",
    "\n",
    "            if np.mean(rewards) >= 200:\n",
    "                print(\"Game cleared in {} games with {}\".format(i + 1, np.mean(rewards)))\n",
    "                break\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
