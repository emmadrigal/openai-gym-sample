{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "This is similar to Q-Learning but in this case the value-action function will be represented by a deep neural network. Neural networks have a greater capacity for generalization and are therefore able to solve problems with a much greater complexity\n",
    "\n",
    "We will use PyTorch to create and train this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "from collections import deque\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network\n",
    "\n",
    "The following simple network with a single layer can learn to balance a pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"DQN Network\n",
    "        Args:\n",
    "            input_dim (int): `state` dimension.\n",
    "                `state` is 2-D tensor of shape (n, input_dim)\n",
    "            output_dim (int): Number of actions.\n",
    "                Q_value is 2-D tensor of shape (n, output_dim)\n",
    "            hidden_dim (int): Hidden dimension in fc layer\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "            torch.nn.BatchNorm1d(hidden_dim),\n",
    "            torch.nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.final = torch.nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns a Q_value\n",
    "        Args:\n",
    "            x (torch.Tensor): `State` 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: Q_value, 2-D tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.final(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use one of the techniques proposed by Mingh et. al. In which we don't only consider the current state/reward but we will also reuse value from previous experiences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\",\n",
    "                        field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \"\"\"Replay memory class\n",
    "        Args:\n",
    "            capacity (int): Max size of this memory\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.cursor = 0\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self,\n",
    "             state: np.ndarray,\n",
    "             action: int,\n",
    "             reward: int,\n",
    "             next_state: np.ndarray,\n",
    "             done: bool) -> None:\n",
    "        \"\"\"Creates `Transition` and insert\n",
    "        Args:\n",
    "            state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            action (int): action index (0 <= action < output_dim)\n",
    "            reward (int): reward value\n",
    "            next_state (np.ndarray): 1-D tensor of shape (input_dim,)\n",
    "            done (bool): whether this state was last step\n",
    "        \"\"\"\n",
    "        if len(self) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "\n",
    "        self.memory[self.cursor] = Transition(state,\n",
    "                                              action, reward, next_state, done)\n",
    "        self.cursor = (self.cursor + 1) % self.capacity\n",
    "\n",
    "    def pop(self, batch_size: int) -> List[Transition]:\n",
    "        \"\"\"Returns a minibatch of `Transition` randomly\n",
    "        Args:\n",
    "            batch_size (int): Size of mini-bach\n",
    "        Returns:\n",
    "            List[Transition]: Minibatch of `Transition`\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length \"\"\"\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following class will represent the agent, it contains the neural network and will be able to decide when to explore the enviroment and when to explote the network. It will also contain the training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int, hidden_dim: int) -> None:\n",
    "        \"\"\"Agent class that choose action and train\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden dimension\n",
    "        \"\"\"\n",
    "        self.dqn = DQN(input_dim, output_dim, hidden_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optim = torch.optim.Adam(self.dqn.parameters())\n",
    "\n",
    "    def _to_variable(self, x: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"torch.Variable syntax helper\n",
    "        Args:\n",
    "            x (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.Tensor: torch variable\n",
    "        \"\"\"\n",
    "        return torch.autograd.Variable(torch.Tensor(x))\n",
    "\n",
    "    def get_action(self, states: np.ndarray, eps: float) -> int:\n",
    "        \"\"\"Returns an action\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D tensor of shape (n, input_dim)\n",
    "            eps (float): ùú∫-greedy for exploration\n",
    "        Returns:\n",
    "            int: action index\n",
    "        \"\"\"\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.choice(self.output_dim)\n",
    "        else:\n",
    "            self.dqn.train(mode=False)\n",
    "            scores = self.get_Q(states)\n",
    "            _, argmax = torch.max(scores.data, 1)\n",
    "            return int(argmax.numpy())\n",
    "\n",
    "    def get_Q(self, states: np.ndarray) -> torch.FloatTensor:\n",
    "        \"\"\"Returns `Q-value`\n",
    "        Args:\n",
    "            states (np.ndarray): 2-D Tensor of shape (n, input_dim)\n",
    "        Returns:\n",
    "            torch.FloatTensor: 2-D Tensor of shape (n, output_dim)\n",
    "        \"\"\"\n",
    "        states = self._to_variable(states.reshape(-1, self.input_dim))\n",
    "        self.dqn.train(mode=False)\n",
    "        return self.dqn(states)\n",
    "\n",
    "    def train(self, Q_pred: torch.FloatTensor, Q_true: torch.FloatTensor) -> float:\n",
    "        \"\"\"Computes `loss` and backpropagation\n",
    "        Args:\n",
    "            Q_pred (torch.FloatTensor): Predicted value by the network,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "            Q_true (torch.FloatTensor): Target value obtained from the game,\n",
    "                2-D Tensor of shape(n, output_dim)\n",
    "        Returns:\n",
    "            float: loss value\n",
    "        \"\"\"\n",
    "        self.dqn.train(mode=True)\n",
    "        self.optim.zero_grad()\n",
    "        loss = self.loss_fn(Q_pred, Q_true)\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions work as helpers for the training.\n",
    "\n",
    "The most important is the `train_helper` in which we implement the $Q$ value-function:\n",
    "\n",
    "$$\n",
    "Q (S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\underset{a}{max} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "\n",
    "def train_helper(agent: Agent, minibatch: List[Transition], gamma: float) -> float:\n",
    "    \"\"\"Prepare minibatch and train them\n",
    "    Args:\n",
    "        agent (Agent): Agent has `train(Q_pred, Q_true)` method\n",
    "        minibatch (List[Transition]): Minibatch of `Transition`\n",
    "        gamma (float): Discount rate of Q_target\n",
    "    Returns:\n",
    "        float: Loss value\n",
    "    \"\"\"\n",
    "    states = np.vstack([x.state for x in minibatch])\n",
    "    actions = np.array([x.action for x in minibatch])\n",
    "    rewards = np.array([x.reward for x in minibatch])\n",
    "    next_states = np.vstack([x.next_state for x in minibatch])\n",
    "    done = np.array([x.done for x in minibatch])\n",
    "\n",
    "    Q_predict = agent.get_Q(states)\n",
    "    Q_target = Q_predict.clone().data.numpy()\n",
    "    Q_target[np.arange(len(Q_target)), actions] = rewards + gamma * np.max(agent.get_Q(next_states).data.numpy(), axis=1) * ~done\n",
    "    Q_target = agent._to_variable(Q_target)\n",
    "\n",
    "    return agent.train(Q_predict, Q_target)\n",
    "\n",
    "\n",
    "def play_episode(env: gym.Env,\n",
    "                 agent: Agent,\n",
    "                 replay_memory: ReplayMemory,\n",
    "                 eps: float,\n",
    "                 batch_size: int) -> int:\n",
    "    \"\"\"Play an epsiode and train\n",
    "    Args:\n",
    "        env (gym.Env): gym environment (CartPole-v0)\n",
    "        agent (Agent): agent will train and get action\n",
    "        replay_memory (ReplayMemory): trajectory is saved here\n",
    "        eps (float): ùú∫-greedy for exploration\n",
    "        batch_size (int): batch size\n",
    "    Returns:\n",
    "        int: reward earned in this episode\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        a = agent.get_action(s, eps)\n",
    "        s2, r, done, info = env.step(a)\n",
    "        env.render()\n",
    "\n",
    "        total_reward += r\n",
    "\n",
    "        if done:\n",
    "            r = -1\n",
    "        replay_memory.push(s, a, r, s2, done)\n",
    "\n",
    "        if len(replay_memory) > batch_size:\n",
    "\n",
    "            minibatch = replay_memory.pop(batch_size)\n",
    "            train_helper(agent, minibatch, GAMMA)\n",
    "\n",
    "        s = s2\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def get_env_dim(env: gym.Env) -> Tuple[int, int]:\n",
    "    \"\"\"Returns input_dim & output_dim\n",
    "    Args:\n",
    "        env (gym.Env): gym Environment (CartPole-v0)\n",
    "    Returns:\n",
    "        int: input_dim\n",
    "        int: output_dim\n",
    "    \"\"\"\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    return input_dim, output_dim\n",
    "\n",
    "\n",
    "def epsilon_annealing(epsiode: int, max_episode: int, min_eps: float) -> float:\n",
    "    \"\"\"Returns ùú∫-greedy\n",
    "    1.0---|\\\n",
    "          | \\\n",
    "          |  \\\n",
    "    min_e +---+------->\n",
    "              |\n",
    "              max_episode\n",
    "    Args:\n",
    "        epsiode (int): Current episode (0<= episode)\n",
    "        max_episode (int): After max episode, ùú∫ will be `min_eps`\n",
    "        min_eps (float): ùú∫ will never go below this value\n",
    "    Returns:\n",
    "        float: ùú∫ value\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_eps - 1.0) / max_episode\n",
    "    return max(slope * epsiode + 1.0, min_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train!\n",
    "\n",
    "Simply run the agent for a given number of episodes until the problem is solved. In this case this is when the cartpole has been balanced for more than 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"CartPole-v0\"\n",
    "HIDDEN_DIM = 12\n",
    "MEMORY_CAPACITY = 5000\n",
    "MAX_EPISODE = 50\n",
    "MIN_EPS = 0.01\n",
    "BATCH_SIZE = 64\n",
    "N_EPISODES = 200\n",
    "\n",
    "try:\n",
    "    env = gym.make(ENV)\n",
    "    env = gym.wrappers.Monitor(env, directory=\"monitors\", force=True)\n",
    "    rewards = deque(maxlen=100)\n",
    "    input_dim, output_dim = get_env_dim(env)\n",
    "    agent = Agent(input_dim, output_dim, HIDDEN_DIM)\n",
    "    replay_memory = ReplayMemory(MEMORY_CAPACITY)\n",
    "\n",
    "    for i in range(N_EPISODES):\n",
    "        eps = epsilon_annealing(i, MAX_EPISODE, MIN_EPS)\n",
    "        r = play_episode(env, agent, replay_memory, eps, BATCH_SIZE)\n",
    "        print(\"[Episode: {:5}] Reward: {:5} ùú∫-greedy: {:5.2f}\".format(i + 1, r, eps))\n",
    "\n",
    "        rewards.append(r)\n",
    "\n",
    "        if len(rewards) == rewards.maxlen:\n",
    "\n",
    "            if np.mean(rewards) >= 200:\n",
    "                print(\"Game cleared in {} games with {}\".format(i + 1, np.mean(rewards)))\n",
    "                break\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
