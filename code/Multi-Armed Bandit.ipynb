{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Arm Bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment\n",
    "\n",
    "The enviroment defines where the agent will interact\n",
    "\n",
    "This enviroment defines the reward as a simple value. Ideally the agent should be able to select the action which always return the best reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class env:\n",
    "    def __init__(self,q_value):\n",
    "        self.true_mean = [1.0, 2.0, 3.0]\n",
    "        self.q_value = q_value\n",
    "        self.arm_count = [1,1,1] \n",
    "\n",
    "    def reward(self,action):\n",
    "        return np.random.randn() + self.true_mean[action]\n",
    "\n",
    "    def update_action_values(self,reward,action):\n",
    "        self.arm_count[action] += 1\n",
    "        step_size = 1/self.arm_count[action]\n",
    "        self.q_value[action] += step_size * (reward -  self.q_value[action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "The agent will interact with the enviroment and try to get at policy $Q$ which can predict the results from the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent:\n",
    "    def __init__ (self,N):\n",
    "        self.arms = 3\n",
    "        self.N = N # number if experiments do by agent   \n",
    "        self.visualiza_data = np.empty(self.N)\n",
    "\n",
    "    def run_experiment(self,env,epsilon=0,c=0):\n",
    "        for iteration in range(self.N):\n",
    "            if epsilon > np.random.random() :\n",
    "                action = np.random.choice(self.arms)\n",
    "            else:\n",
    "                max_action_values = [i for i in range(self.arms)\n",
    "                       if env.q_value[i] == np.max(env.q_value)]\n",
    "                action = np.random.choice(max_action_values)\n",
    "\n",
    "                reward = env.reward(action)    \n",
    "                env.update_action_values(reward,action)\n",
    "                self.visualiza_data[iteration] = reward\n",
    "\n",
    "            cumulative_average = np.cumsum(self.visualiza_data)/(np.arange(self.N) + 1 )\n",
    "        print()\n",
    "        return cumulative_average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiment\n",
    "\n",
    "The agent \n",
    "\n",
    "There is an epsilon value which will present how likely is the agent to explore actions which aren't the optimal one presented by the current policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 2000\n",
    "\n",
    "q_value = [1.0,2.0,1.5]\n",
    "exp_1 = agent(N).run_experiment( env(q_value), 0.1)\n",
    "exp_2 = agent(N).run_experiment( env(q_value), 0.05)\n",
    "exp_3 = agent(N).run_experiment( env(q_value), 0.01)\n",
    "exp_4 = agent(N).run_experiment( env(q_value), 0)\n",
    "plt.plot(exp_1 , label = \"eps = 0.1\")\n",
    "plt.plot(exp_2 , label = \"eps = 0.05\")\n",
    "plt.plot(exp_3 , label = \"eps = 0.01\")\n",
    "plt.plot(exp_4 , label = \"greedy\")\n",
    "plt.legend()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
