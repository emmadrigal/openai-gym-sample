{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Introduction\n",
    "\n",
    "Reinforcement learning is learning what to do to maximize a numerical\n",
    "reward signal. The learner is not told which actions to take, but must\n",
    "discover which actions yield the most reward by trying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import SVG\n",
    "SVG(filename='Reinforcement_learning_diagram.svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not supervised learning:\n",
    "* No labeled data\n",
    "* Examples of desired behaviour of all situations are difficult to find\n",
    "* Agent must be able to learn from its own experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not unsupervised learning:\n",
    "* Not looking to find a hidden structure but to maximise a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "### Agent\n",
    "\n",
    "* Active participant on the enviroment.\n",
    "* Seeks to achieve a given goal.\n",
    "\n",
    "### Enviroment\n",
    "* Place where the agent is active.\n",
    "* Has a defined goal.\n",
    "* Progress towards the goal can be shown.\n",
    "\n",
    "### Policy \n",
    "* Mapping from the perceived states from the enviroment to the possible actions to advance to a new state.\n",
    "* This is updated on an immediate reward signal which leads towards an end-goal.\n",
    "\n",
    "### Reward and Value\n",
    "* A reward is an immediate goal.\n",
    "* A value function specifies what is good in the long run.\n",
    "* The goal is the highest value not the highest reward.\n",
    "\n",
    "### Enviroment Model\n",
    "* This is something that mimics the vehaviour of the model.\n",
    "* Models might be able to predict the next state and action based on the current values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "### Monte-Carlo\n",
    "\n",
    "Monte Carlo methods are ways of solving the reinforcement learning problem based on averaging sample returns.\n",
    "\n",
    "In Monte Carlo we play an episode of the enviroment starting by some random state till the end, record the states, actions and rewards that we encountered then compute the the policy for each state we passed through. The final result can be averaged for multiple runs,\n",
    "\n",
    "In Monte Carlo there is no guarantee that we will visit all the possible states, another weakness of this method is that we need to wait until the enviroment ends to be able to update our policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "In Q-Learning we are looking for an action-value function $Q$ directly approximates $q_*$ which is the optimal action-value function:\n",
    "\n",
    "$$\n",
    "Q (S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha \\left[ R_{t+1} + \\gamma \\underset{a}{max} Q(S_{t+1}, a) - Q(S_t, A_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Learning\n",
    "\n",
    "This is an extension of Q-Learning, that combines it with artificial\n",
    "neural networks. Minh et al showed how a single\n",
    "reinforcement learning agent can achieve high levels of performance in\n",
    "many different problems without relying on different problem-specific\n",
    "feature sets.\n",
    "\n",
    "![SegmentLocal](atari_2.gif \"segment\")\n",
    "\n",
    "Taken from: https://ai.googleblog.com/2020/03/introducing-dreamer-scalable.html\n",
    "\n",
    "They also implemented some new techniques to further increase performance:\n",
    "\n",
    "* Updating based more than a single event. Values are saved from multiple runs an then feed as a batch.\n",
    "* By waiting until the multiple runs are done, prediction of the next state is not needed, this is closer to supervised learning:\n",
    "\n",
    "\n",
    "$$\n",
    "w_{t+1}=w_t+\\alpha[R_{t+1}+\\gamma \\underset{a}{max} \\tilde{q}(S_{t+1},a,w_t)-\\hat{q}(S_t,A_t,w_t)]\\nabla_{w_t}\\hat{q}(S_t,A_t,w_t).\n",
    "$$\n",
    "\n",
    "* Error was clipped from $[-1, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic\n",
    "\n",
    "The principal idea is to split the deep q-learning model in two:\n",
    "* One for computing an action based on a state and\n",
    "* another one to produce the $Q$ values of the action.\n",
    "\n",
    "The actor takes as input the state and outputs the best action.\n",
    "The critic, on the other hand, evaluates the action by\n",
    "computing the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other\n",
    "\n",
    "Other algorithms include:\n",
    "\n",
    "* Double DQN\n",
    "* Deep Deterministic Policy Gradient\n",
    "* SARSA\n",
    "* Proximal Policy Optimization\n",
    "* Trust-Region Policy Optimization\n",
    "* Deterministic Policy Gradient\n",
    "* Generative Adversarial Imitation Learning\n",
    "* Hindsight Experience Replay\n",
    "* Soft Actor Critic\n",
    "* Twin Delayed DDPG\n",
    "* Trust Region Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frameworks\n",
    "\n",
    "### RLLib\n",
    "\n",
    "RLlib is an open-source library for reinforcement learning that offers\n",
    "both high scalability and a unified API for a variety of\n",
    "applications. RLlib natively supports TensorFlow, TensorFlow Eager,\n",
    "and PyTorch, but most of its internals are framework agnostic.\n",
    "\n",
    "* Distributed Prioritized Experience Replay (Ape-X)\n",
    "* Importance Weighted Actor-Learner Architecture (IMPALA)\n",
    "* Asynchronous Proximal Policy Optimization (APPO)\n",
    "* Decentralized Distributed Proximal Policy Optimization (DD-PPO)\n",
    "* Single-Player AlphaZero (contrib/AlphaZero)\n",
    "* Advantage Actor-Critic (A2C, A3C)\n",
    "* Deep Deterministic Policy Gradients (DDPG, TD3)\n",
    "* Deep Q Networks (DQN, Rainbow, Parametric DQN)\n",
    "* Proximal Policy Optimization (PPO)\n",
    "* Soft Actor Critic (SAC)\n",
    "* Augmented Random Search (ARS)\n",
    "* QMIX Monotonic Value Factorisation (QMIX, VDN, IQN)\n",
    "* Multi-Agent Deep Deterministic Policy Gradient (contrib/MADDPG)\n",
    "* Advantage Re-Weighted Imitation Learning (MARWIL)\n",
    "* Linear Upper Confidence Bound (contrib/LinUCB)\n",
    "* Linear Thompson Sampling (contrib/LinTS)\n",
    "\n",
    "### SLM Lab\n",
    "\n",
    "SLM Lab is a software framework for reproducible reinforcement\n",
    "learning research. It enables easy development of RL algorithms using\n",
    "modular components and file-based configuration. It also enables\n",
    "flexible experimentation completed with hyperparameter search, result\n",
    "analysis and benchmark results.\n",
    "\n",
    "* SARSA\n",
    "* DQN (Deep Q-Network)\n",
    "* Double-DQN, Dueling-DQN, PER (Prioritized Experience Replay)\n",
    "* REINFORCE\n",
    "* A2C (Advantage Actor-Critic) with GAE & n-step\n",
    "* PPO (Proximal Policy Optimization)\n",
    "* SAC (Soft Actor-Critic)\n",
    "* SIL (Self Imitation Learning)\n",
    "\n",
    "### TF Agents\n",
    "\n",
    "TF-Agents is a relatively new but up-and-coming tool, again, from\n",
    "Google.  TF-Agents, while newer, is typically seen as more robust and\n",
    "mature. It is a framework designed for notebooks and that makes it\n",
    "perfect for trying out various configurations, hyperparameters, or\n",
    "environments.\n",
    "\n",
    "* Deep Q Learning\n",
    "* Double DQN\n",
    "* Twin Delayed DDPG\n",
    "* REINFORCE\n",
    "* Proximal Policy Optimization\n",
    "* Soft Actor Critic\n",
    "\n",
    "### KerasRL\n",
    "\n",
    "Implements some state-of-the art deep reinforcement learning\n",
    "algorithms in Python and seamlessly integrates with the deep learning\n",
    "library Keras.\n",
    "\n",
    "Furthermore, keras-rl works with OpenAI Gym out of the box. This means\n",
    "that evaluating and playing around with different algorithms is easy.\n",
    "\n",
    "* Deep Q Learning\n",
    "* Double DQN\n",
    "* Deep Determenistic Policy Gradient\n",
    "* Continous DQN\n",
    "* Cross-Entropy Method\n",
    "* Dueling Network DQN\n",
    "* Deep SARSA\n",
    "\n",
    "### TensorForce\n",
    "\n",
    "Tensorforce is an open-source deep reinforcement\n",
    "learning framework, with an emphasis on modularized flexible library\n",
    "design and straightforward usability for applications in research and\n",
    "practice.\n",
    "\n",
    "* Proximal Policy Optimization\n",
    "* Trust-Region Policy Optimization\n",
    "* Deterministic Policy Gradient\n",
    "* Deep Q-Network\n",
    "* Double DQN\n",
    "* Dueling DQN\n",
    "* Actor-Critic\n",
    "* Advantage Actor-Critic\n",
    "\n",
    "### Stable Baselines\n",
    "\n",
    "Stable Baselines, a set of implementations of\n",
    "Reinforcement Learning (RL) algorithms with a common interface, based\n",
    "on OpenAI Baselines.\n",
    "\n",
    "* Advantage Actor Critic\n",
    "* Actor Critic with Experience Replay\n",
    "* Actor Critic using Kronecker Factored Trust Region\n",
    "* Deep Determenistic Policy Gradient\n",
    "* Deep Q Learning\n",
    "* Generative Adversarial Imitation Learning\n",
    "* Hindsight Experience Relay\n",
    "* Proximal Policy Optimization\n",
    "* Soft Actor Critic\n",
    "* Twin Delayed DDPG\n",
    "* Trust Region Policy Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "### Batch Off-line and Off-Policy Training\n",
    "\n",
    "Many systems cannot be trained on directly and need to learn from\n",
    "fixed logs of the system’s behavior. Most times, we are deploying an\n",
    "RL approach to replace a previous control system, and logs from that\n",
    "policy are available.\n",
    "\n",
    "### Learning On the Real System from Limited Samples\n",
    "\n",
    "Actual systems do not have separate training and evaluation\n",
    "environments. All training data comes from the actual system, and the\n",
    "agent cannot have a separate exploration policy during training as its\n",
    "exploratory actions do not come for free.\n",
    "\n",
    "### Satisfying Safety Constrains\n",
    "\n",
    "Almost all physical systems can destroy or degrade themselves and\n",
    "their environment. Considering these systems’ safety is necessary for\n",
    "controlling them. Safety is important during system operation, but\n",
    "also during exploratory learning phases.\n",
    "\n",
    "### Partial Observability and Non-Stationarity\n",
    "\n",
    "Almost all real systems where we would want to deploy reinforcement\n",
    "learning are partially observable. For example, on a physical system,\n",
    "we likely do not have observations of the wear and tear on motors or\n",
    "joints, or the amount of buildup in pipes or vents. \n",
    "\n",
    "### Unspecified and Multi-Objective Reward Functions\n",
    "\n",
    "Reinforcement learning frames policy learning through the lens of\n",
    "optimizing a global reward function, yet most systems have\n",
    "multidimensional costs to be minimized. Most times, system or product\n",
    "owners do not have a clear picture of what they want to optimize. \n",
    "\n",
    "### Explainability\n",
    "\n",
    "In the event of policy errors, being able to understand the\n",
    "error’s origins a posteriori is essential.\n",
    "\n",
    "### Multi-task Learning\n",
    "\n",
    "An agent should be able to perform many types\n",
    "of tasks, rather than specializing in just one. The core of this\n",
    "challenge is scalability. It should not take 1000 times as many\n",
    "samples or hours of computation time to learn 1000 different tasks\n",
    "than to learn one single task.\n",
    "\n",
    "### Learning to Remember\n",
    "\n",
    "An observation only captures a minor part\n",
    "of the full environment state that determines the best action. In such\n",
    "partially observable environments, an agent has to take into account\n",
    "not just the current observation, but also past observations to\n",
    "determine the best action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "\n",
    "### Resource Management in Computer Clusters\n",
    "\n",
    "In \"Resource management with deep reinforcement learning\" the authors showed how to use RL\n",
    "to automatically learn to allocate and schedule computer resources to\n",
    "waiting jobs, with the objective to minimize the average job slowdown.\n",
    "\n",
    "### Traffic Light Control\n",
    "\n",
    "In \"Reinforcement learning-based multi-agent system for network traffic signal control\", researchers tried to design a traffic\n",
    "light controller to solve the congestion problem. Tested only on\n",
    "simulated environment though, their methods showed superior results\n",
    "than traditional methods and shed a light on the potential uses of\n",
    "multi-agent RL in designing traffic system.\n",
    "\n",
    "\n",
    "![Five-intersection traffic network, taken from: https://web.eecs.utk.edu/~ielhanan/Papers/IET_ITS_2010.pdf](five_intersection.png)\n",
    "\n",
    "### Robotics\n",
    "\n",
    "In \"End-to-end training of deep visuomotor policies\" trained a robot to learn policies to map raw videno images to the\n",
    "robot’s actions. They fed the RGB images to a CNN and outputs were the\n",
    "motor torques. The RL component was the guided policy search to\n",
    "generate training data that came from its own state distribution.\n",
    "\n",
    "### Personalized Recommendations\n",
    "\n",
    "In \"DRN: A deep reinforcement learning framework for news recommendation\" the authors have applied RL in news recommendation system to combat the\n",
    "problems of changing dynamic of news, users get bored and Click\n",
    "Through Rate cannot show the retention rate of users.\n",
    "\n",
    "They constructed four categories of\n",
    "features:\n",
    "* User features\n",
    "* Context features as the state features of the environment\n",
    "* User-news features\n",
    "* News features as the action features.\n",
    "\n",
    "The four features were input to the Deep Q-Network (DQN) to calculate\n",
    "the Q-value. The agent chose a list of news to recommend based on the\n",
    "Q-value, and the user’s click on the news was a part of the reward the\n",
    "RL agent received."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
